{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(11)  # reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        data = pickle.load(fo, encoding='bytes')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the CIFAR-10\n",
    "def load_CIFAR10(pos, n_chunks=1):\n",
    "    Xtr = []\n",
    "    Ytr = []\n",
    "    for i in range(n_chunks):\n",
    "        train = unpickle(pos + '/data_batch_{0}'.format(i + 1))\n",
    "        Xtr.extend(train[b'data'])\n",
    "        Ytr.extend(train[b'labels'])\n",
    "        test = unpickle(pos + '/test_batch')\n",
    "        Xte = test[b'data']\n",
    "        Yte = test[b'labels']\n",
    "    return np.array(Xtr), np.array(Ytr), np.array(Xte), np.array(Yte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expresses the label data in one-hot encoding.\n",
    "def onehot_encoding (Ytr, Yte):\n",
    "    Ytr_onehot = np.zeros((Ytr.size, 10))\n",
    "    Yte_onehot = np.zeros((Yte.size, 10))\n",
    "    for i in range(Ytr.size):\n",
    "        Ytr_onehot[i][Ytr[i]] = 1\n",
    "    for i in range(Yte.size):\n",
    "        Yte_onehot[i][Yte[i]] = 1\n",
    "    return Ytr_onehot, Yte_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the train and test data\n",
    "Xtr, Ytr, Xte, Yte = load_CIFAR10('cifar-10-batches-py', 5)\n",
    "                                 \n",
    "# image data, each data size is 32*32*3\n",
    "Xtr = Xtr.reshape(50000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
    "Xte = Xte.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
    "\n",
    "# label data of train and test data, label data is represented by one-hot encoding\n",
    "Ytr_onehot, Yte_onehot = onehot_encoding(Ytr, Yte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "dropout_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 여기서부터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"conv11/Relu:0\", shape=(?, 32, 32, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "### Implement the layers of CNNs ###\n",
    "\n",
    "with tf.name_scope('conv11'):\n",
    "    W11 = tf.Variable(tf.random_normal([3,3,3,3], stddev = 0.01))\n",
    "    L11 = tf.nn.conv2d(X, W11, strides = [1,1,1,1], padding = 'SAME')\n",
    "    L11 = tf.nn.relu(L11)\n",
    "    #L1 = tf.contrib.layers.batch_norm(L1)\n",
    "    #L1 = tf.nn.max_pool(L1, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')\n",
    "    #L1 = tf.nn.dropout(L1, keep_prob=dropout_prob)\n",
    "    print(L11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"conv12/MaxPool:0\", shape=(?, 16, 16, 32), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('conv12'):\n",
    "    W12 = tf.Variable(tf.random_normal([3,3,3,32], stddev = 0.01))\n",
    "    L12 = tf.nn.conv2d(L11, W12, strides = [1,1,1,1], padding = 'SAME')\n",
    "    L12 = tf.nn.relu(L12)\n",
    "    L12 = tf.contrib.layers.batch_norm(L12)\n",
    "    L12 = tf.nn.max_pool(L12, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')\n",
    "    #L1 = tf.nn.dropout(L1, keep_prob=dropout_prob)\n",
    "    print(L12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"conv21/BatchNorm/FusedBatchNorm:0\", shape=(?, 16, 16, 64), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('conv21'):\n",
    "    W21 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n",
    "    L21 = tf.nn.conv2d(L12, W21, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    L21 = tf.nn.relu(L21)\n",
    "    L21 = tf.contrib.layers.batch_norm(L21)\n",
    "\n",
    "    print(L21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"conv22/MaxPool:0\", shape=(?, 8, 8, 128), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('conv22'):\n",
    "    W22 = tf.Variable(tf.random_normal([3, 3, 64, 128], stddev=0.01))\n",
    "    L22 = tf.nn.conv2d(L21, W22, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    L22 = tf.nn.relu(L22)\n",
    "    L22 = tf.contrib.layers.batch_norm(L22)\n",
    "    L22 = tf.nn.max_pool(L22, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    print(L22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"conv31/BatchNorm/FusedBatchNorm:0\", shape=(?, 8, 8, 256), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('conv31'):\n",
    "    W31 = tf.Variable(tf.random_normal([3, 3, 128, 256], stddev=0.01))\n",
    "    L31 = tf.nn.conv2d(L22, W31, strides = [1,1,1,1], padding = \"SAME\")\n",
    "    L31 = tf.nn.relu(L31)\n",
    "    L31 = tf.contrib.layers.batch_norm(L31)\n",
    "    \n",
    "    #L3 = tf.nn.dropout(L3, keep_prob=dropout_prob)\n",
    "    print(L31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"conv32/MaxPool:0\", shape=(?, 4, 4, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('conv32'):\n",
    "    W32 = tf.Variable(tf.random_normal([3, 3, 256, 512], stddev=0.01))\n",
    "    L32 = tf.nn.conv2d(L31, W32, strides = [1,1,1,1], padding = \"SAME\")\n",
    "    L32 = tf.nn.relu(L32)\n",
    "    L32 = tf.contrib.layers.batch_norm(L32)\n",
    "    L32 = tf.nn.max_pool(L32, ksize = [1,2,2,1], strides = [1,2,2,1], padding='SAME')\n",
    "    #L3 = tf.nn.dropout(L3, keep_prob=dropout_prob)\n",
    "    print(L32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"fc1/BatchNorm/Reshape_1:0\", shape=(?, 256), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('fc1'):\n",
    "    W4 = tf.Variable(tf.random_normal([4*4*512, 256], stddev=0.01))\n",
    "    L4 = tf.reshape(L32, [-1, 4*4*512])\n",
    "    L4 = tf.matmul(L4, W4)\n",
    "    b4 = tf.Variable(tf.random_normal([256]))\n",
    "    L4 = tf.nn.relu(L4 + b4)\n",
    "    L4 = tf.contrib.layers.batch_norm(L4)\n",
    "    \n",
    "    # dropout\n",
    "    #L4 = tf.nn.dropout(L4, keep_prob=dropout_prob)\n",
    "    print(L4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"fc2/BatchNorm/Reshape_1:0\", shape=(?, 64), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('fc2'):\n",
    "    W5 = tf.Variable(tf.random_normal([256, 64], stddev=0.01))\n",
    "    L5 = tf.matmul(L4, W5)\n",
    "    b5 = tf.Variable(tf.random_normal([64]))\n",
    "    L5 = tf.nn.relu(L5 + b5)\n",
    "    L5 = tf.contrib.layers.batch_norm(L5)\n",
    "    \n",
    "    # dropout\n",
    "    #L5 = tf.nn.dropout(L5, keep_prob=dropout_prob)\n",
    "    print(L5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"fc3/add:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('fc3'):\n",
    "    W6 = tf.Variable(tf.random_normal([64, 10],stddev= 0.01))\n",
    "    b6 = tf.Variable(tf.random_normal([10]))\n",
    "    model = tf.matmul(L5, W6) + b6\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('accuracy'):\n",
    "    correct_prediction = tf.equal(tf.argmax(model, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 여기까지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 50\n",
    "batch_size = 500\n",
    "test_size = 1000\n",
    "total_batch = int(len(Xtr) / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost function, you can change the implementation\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 여기서부터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************Learning started********************\n",
      "Epoch:  0001\n",
      "Average cost =  1.8067516684532166\n",
      "Accuracy =  0.671999990940094\n",
      "\n",
      "Epoch:  0002\n",
      "Average cost =  0.831078726053238\n",
      "Accuracy =  0.7760000228881836\n",
      "\n",
      "Epoch:  0003\n",
      "Average cost =  0.5677063572406769\n",
      "Accuracy =  0.7590000033378601\n",
      "\n",
      "Epoch:  0004\n",
      "Average cost =  0.4171694865822792\n",
      "Accuracy =  0.7940000295639038\n",
      "\n",
      "Epoch:  0005\n",
      "Average cost =  0.30469118908047677\n",
      "Accuracy =  0.7960000038146973\n",
      "\n",
      "Epoch:  0006\n",
      "Average cost =  0.2072998932003975\n",
      "Accuracy =  0.7870000004768372\n",
      "\n",
      "Epoch:  0007\n",
      "Average cost =  0.11982771694660187\n",
      "Accuracy =  0.777999997138977\n",
      "\n",
      "Epoch:  0008\n",
      "Average cost =  0.06613451005890965\n",
      "Accuracy =  0.8080000281333923\n",
      "\n",
      "Epoch:  0009\n",
      "Average cost =  0.03361167737282813\n",
      "Accuracy =  0.8220000267028809\n",
      "\n",
      "Epoch:  0010\n",
      "Average cost =  0.014603421189822257\n",
      "Accuracy =  0.8270000219345093\n",
      "\n",
      "Epoch:  0011\n",
      "Average cost =  0.005255046782549471\n",
      "Accuracy =  0.8460000157356262\n",
      "\n",
      "Epoch:  0012\n",
      "Average cost =  0.0024680037004873156\n",
      "Accuracy =  0.8420000076293945\n",
      "\n",
      "Epoch:  0013\n",
      "Average cost =  0.001659505305578932\n",
      "Accuracy =  0.8270000219345093\n",
      "\n",
      "Epoch:  0014\n",
      "Average cost =  0.0014157679281197489\n",
      "Accuracy =  0.8420000076293945\n",
      "\n",
      "Epoch:  0015\n",
      "Average cost =  0.001139572989777662\n",
      "Accuracy =  0.8410000205039978\n",
      "\n",
      "Epoch:  0016\n",
      "Average cost =  0.000952090107020922\n",
      "Accuracy =  0.8550000190734863\n",
      "\n",
      "Epoch:  0017\n",
      "Average cost =  0.0008302648685639724\n",
      "Accuracy =  0.8460000157356262\n",
      "\n",
      "Epoch:  0018\n",
      "Average cost =  0.0007372686435701325\n",
      "Accuracy =  0.8389999866485596\n",
      "\n",
      "Epoch:  0019\n",
      "Average cost =  0.0006600504979724065\n",
      "Accuracy =  0.8700000047683716\n",
      "\n",
      "Epoch:  0020\n",
      "Average cost =  0.0005952957918634638\n",
      "Accuracy =  0.8629999756813049\n",
      "\n",
      "Epoch:  0021\n",
      "Average cost =  0.0005398191881249659\n",
      "Accuracy =  0.8529999852180481\n",
      "\n",
      "Epoch:  0022\n",
      "Average cost =  0.0004917288097203709\n",
      "Accuracy =  0.8460000157356262\n",
      "\n",
      "Epoch:  0023\n",
      "Average cost =  0.00044964382686885074\n",
      "Accuracy =  0.8410000205039978\n",
      "\n",
      "Epoch:  0024\n",
      "Average cost =  0.00041249010129831734\n",
      "Accuracy =  0.8489999771118164\n",
      "\n",
      "Epoch:  0025\n",
      "Average cost =  0.00037949920137180014\n",
      "Accuracy =  0.8510000109672546\n",
      "\n",
      "Epoch:  0026\n",
      "Average cost =  0.00035004518809728325\n",
      "Accuracy =  0.8309999704360962\n",
      "\n",
      "Epoch:  0027\n",
      "Average cost =  0.0003235539549496025\n",
      "Accuracy =  0.8489999771118164\n",
      "\n",
      "Epoch:  0028\n",
      "Average cost =  0.00029962109256302937\n",
      "Accuracy =  0.8410000205039978\n",
      "\n",
      "Epoch:  0029\n",
      "Average cost =  0.00027790665335487575\n",
      "Accuracy =  0.8270000219345093\n",
      "\n",
      "Epoch:  0030\n",
      "Average cost =  0.000258145884872647\n",
      "Accuracy =  0.8659999966621399\n",
      "\n",
      "Epoch:  0031\n",
      "Average cost =  0.0002401336291222833\n",
      "Accuracy =  0.8339999914169312\n",
      "\n",
      "Epoch:  0032\n",
      "Average cost =  0.00022368653560988605\n",
      "Accuracy =  0.8330000042915344\n",
      "\n",
      "Epoch:  0033\n",
      "Average cost =  0.00020863653291598893\n",
      "Accuracy =  0.8529999852180481\n",
      "\n",
      "Epoch:  0034\n",
      "Average cost =  0.00019485199693008325\n",
      "Accuracy =  0.8450000286102295\n",
      "\n",
      "Epoch:  0035\n",
      "Average cost =  0.00018222186423372478\n",
      "Accuracy =  0.8289999961853027\n",
      "\n",
      "Epoch:  0036\n",
      "Average cost =  0.00017060911108274013\n",
      "Accuracy =  0.8569999933242798\n",
      "\n",
      "Epoch:  0037\n",
      "Average cost =  0.00015990847896318883\n",
      "Accuracy =  0.8349999785423279\n",
      "\n",
      "Epoch:  0038\n",
      "Average cost =  0.00015002419168013147\n",
      "Accuracy =  0.8550000190734863\n",
      "\n",
      "Epoch:  0039\n",
      "Average cost =  0.0001408742109197192\n",
      "Accuracy =  0.8489999771118164\n",
      "\n",
      "Epoch:  0040\n",
      "Average cost =  0.0001323872187640518\n",
      "Accuracy =  0.8379999995231628\n",
      "\n",
      "Epoch:  0041\n",
      "Average cost =  0.00012449991823814343\n",
      "Accuracy =  0.8460000157356262\n",
      "\n",
      "Epoch:  0042\n",
      "Average cost =  0.00011715942811861168\n",
      "Accuracy =  0.8349999785423279\n",
      "\n",
      "Epoch:  0043\n",
      "Average cost =  0.00011032070797227789\n",
      "Accuracy =  0.8560000061988831\n",
      "\n",
      "Epoch:  0044\n",
      "Average cost =  0.00010393986776762177\n",
      "Accuracy =  0.8379999995231628\n",
      "\n",
      "Epoch:  0045\n",
      "Average cost =  9.798089748073835e-05\n",
      "Accuracy =  0.8529999852180481\n",
      "\n",
      "Epoch:  0046\n",
      "Average cost =  9.241183121048379e-05\n",
      "Accuracy =  0.8379999995231628\n",
      "\n",
      "Epoch:  0047\n",
      "Average cost =  8.719806450244505e-05\n",
      "Accuracy =  0.8410000205039978\n",
      "\n",
      "Epoch:  0048\n",
      "Average cost =  8.231581385189202e-05\n",
      "Accuracy =  0.8309999704360962\n",
      "\n",
      "Epoch:  0049\n",
      "Average cost =  7.773868455842602e-05\n",
      "Accuracy =  0.8510000109672546\n",
      "\n",
      "Epoch:  0050\n",
      "Average cost =  7.344661593378987e-05\n",
      "Accuracy =  0.8389999866485596\n",
      "\n",
      "********************Learning finished********************\n"
     ]
    }
   ],
   "source": [
    "### Implement the train process ###\n",
    "print(\"*\"*20 + 'Learning started' + \"*\"*20)\n",
    "\n",
    "#for epoch in range(training_epochs):\n",
    "for epoch in range(training_epochs):\n",
    "    total_cost = 0\n",
    "    \n",
    "    for batch_idx in range(total_batch):\n",
    "        batch_xs = Xtr[batch_size * batch_idx : batch_size * (batch_idx+1)]\n",
    "        batch_ys = Ytr_onehot[batch_size * batch_idx : batch_size * (batch_idx+1)]\n",
    "            \n",
    "        feed_dicts = {X: batch_xs, Y: batch_ys, dropout_prob: 0.7}\n",
    "        _, cost_val = sess.run([optimizer, cost], feed_dict = feed_dicts) #optimizer는 필요가 없어서 _에 assign\n",
    "        total_cost += cost_val\n",
    "        \n",
    "    test_indices = np.arange(len(Yte_onehot))\n",
    "    np.random.shuffle(test_indices)\n",
    "    test_indices = test_indices[:test_size]\n",
    "    \n",
    "    test_xs = Xte[test_indices]\n",
    "    test_ys = Yte_onehot[test_indices]\n",
    "    feed_dicts = {X: test_xs, Y: test_ys, dropout_prob: 1}\n",
    "#     feed_dicts = {X: Xte, Y: Yte_onehot, dropout_prob: 1}\n",
    "    acc = sess.run(accuracy, feed_dict = feed_dicts )\n",
    "        \n",
    "    print(\"Epoch: \", \"%04d\" %(epoch + 1))\n",
    "    print(\"Average cost = \", \"{}\".format(total_cost/total_batch))\n",
    "    print(\"Accuracy = \", \"{}\\n\".format(acc))\n",
    "    \n",
    "    \n",
    "print(\"*\"*20 + 'Learning finished' + \"*\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the test process ###\n",
    "\n",
    "print('Accuracy', sess.run(accuracy, feed_dict = {X: Xte, Y: Yte_onehot, dropout_prob: 1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.858\n",
      "0.856\n",
      "0.842\n",
      "0.824\n",
      "0.82\n",
      "0.838\n",
      "0.838\n",
      "0.83\n",
      "0.862\n",
      "0.842\n",
      "0.848\n",
      "0.826\n",
      "0.846\n",
      "0.834\n",
      "0.854\n",
      "0.826\n",
      "0.84\n",
      "0.84\n",
      "0.86\n",
      "0.836\n",
      "\n",
      "Avg acc: 0.8409999936819077\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "\n",
    "total_acc = 0\n",
    "total_batch = int(len(Yte_onehot)/batch_size)\n",
    "\n",
    "for idx in range(total_batch):\n",
    "    curr_acc = sess.run(accuracy, feed_dict={X: Xte[batch_size*idx: batch_size*(idx+1)],\\\n",
    "                                             Y: Yte_onehot[batch_size*idx: batch_size*(idx+1)], dropout_prob: 1})\n",
    "    \n",
    "    total_acc += curr_acc\n",
    "    print(curr_acc)\n",
    "\n",
    "print(\"\\nAvg acc: {}\".format(total_acc/total_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 여기까지"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
